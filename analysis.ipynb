{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## machine-learners\n",
    "\n",
    "In this project we try out different models and feature sets to see what works best when trying to predict the sentiment of tweets about stocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We read in the data and organize it into train & test.\n",
    "train_examples = pd.read_csv('sent_train.csv').to_numpy()\n",
    "X_train = train_examples[:,0]\n",
    "y_train = train_examples[:,1].astype('int')\n",
    "\n",
    "test_examples = pd.read_csv('sent_valid.csv').to_numpy()\n",
    "X_test = test_examples[:,0]\n",
    "y_test = test_examples[:,1].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step I: Normalize\n",
    "# Step II: Tokenize\n",
    "# Step III: Get Features\n",
    "# Step IV: Train Model\n",
    "# Step V: Test to Get F1-Score & Other Measures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.55083179 0.66666667 0.88155922]\n",
      "0.8036013400335008\n"
     ]
    }
   ],
   "source": [
    "# The baseline. We don't try and do anything fancy here. We use this to compare with any improvements we make. \n",
    "\n",
    "def normalize_baseline(text):\n",
    "    text = text.lower()\n",
    "    # Remove links. \n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# This returns:\n",
    "#  (1) A map that maps a token to a spot in the feature array. \n",
    "def create_baseline_feature_dict(training_examples):\n",
    "    curr_pos = 0\n",
    "    features = {}\n",
    "\n",
    "    for example in training_examples:\n",
    "        tokens = normalize_baseline(example)\n",
    "\n",
    "        for token in tokens:\n",
    "            if token not in features:\n",
    "                features[token] = curr_pos\n",
    "                curr_pos += 1\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Our baseline features are arrays with binary (0 or 1) values where we note whether or not a particular word has been seen. \n",
    "#\n",
    "# If the secord argument is \n",
    "def featurize_baseline(examples, feature_dict):\n",
    "    feature_vecs = np.zeros((len(examples), len(feature_dict)))\n",
    "\n",
    "    for i in range(len(examples)):\n",
    "        tokens = normalize_baseline(examples[i])\n",
    "\n",
    "        for j in range(len(tokens)):\n",
    "            if tokens[j] in feature_dict:\n",
    "                feature_idx = feature_dict[tokens[j]]\n",
    "                feature_vecs[i][feature_idx] = 1\n",
    "\n",
    "    return feature_vecs\n",
    "\n",
    "feature_dict = create_baseline_feature_dict(X_train)\n",
    "X_train_baseline = featurize_baseline(X_train, feature_dict)\n",
    "X_test_baseline = featurize_baseline(X_test, feature_dict)\n",
    "mnb = MultinomialNB()\n",
    "y_pred = mnb.fit(X_train_baseline, y_train).predict(X_test_baseline)\n",
    "\n",
    "print(f1_score(y_test, y_pred, average=None))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.57471264 0.66597725 0.871875  ]\n",
      "0.7922948073701842\n"
     ]
    }
   ],
   "source": [
    "# Here we use counts as a feature. \n",
    "\n",
    "# We reuse the normalization function\n",
    "\n",
    "def create_count_feature_dict(training_examples):\n",
    "    curr_pos = 0\n",
    "    features = {}\n",
    "\n",
    "    for example in training_examples:\n",
    "        tokens = normalize_baseline(example)\n",
    "\n",
    "        for token in tokens:\n",
    "            if token not in features:\n",
    "                features[token] = curr_pos\n",
    "                curr_pos += 1\n",
    "    \n",
    "    features['UNKNOWN'] = curr_pos\n",
    "\n",
    "    return features\n",
    "\n",
    "def featurize_count(examples, feature_dict):\n",
    "    feature_vecs = np.zeros((len(examples), len(feature_dict)))\n",
    "\n",
    "    for i in range(len(examples)):\n",
    "        tokens = normalize_baseline(examples[i])\n",
    "\n",
    "        for j in range(len(tokens)):\n",
    "            if tokens[j] in feature_dict:\n",
    "                feature_idx = feature_dict[tokens[j]]\n",
    "                feature_vecs[i][feature_idx] += 1\n",
    "            else:\n",
    "                feature_vecs[i][len(feature_dict) - 1] += 1\n",
    "\n",
    "    return feature_vecs\n",
    "\n",
    "feature_dict_count = create_count_feature_dict(X_train)\n",
    "X_train_count = featurize_count(X_train, feature_dict_count)\n",
    "X_test_count = featurize_count(X_test, feature_dict_count)\n",
    "mnb = MultinomialNB()\n",
    "y_pred = mnb.fit(X_train_count, y_train).predict(X_test_count)\n",
    "\n",
    "print(f1_score(y_test, y_pred, average=None))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
